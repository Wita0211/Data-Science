# -*- coding: utf-8 -*-
"""UAS DS Sentimen Komentar Yt

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r82yIybK6QG1NieWLmJT3Na_K0K3guF3

**Import Library**
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import re
import string
import nltk
from nltk.sentiment.vader import  SentimentIntensityAnalyzer
from googleapiclient.discovery import build
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score
from sklearn import metrics
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics  import confusion_matrix
from sklearn.metrics import classification_report,confusion_matrix

"""**Scrapping Data**"""

!pip install youtube-scraper

"""**Import Paket**"""

def video_comments(video_id):
	# empty list for storing reply
	replies = []

	# creating youtube resource object
	youtube = build('youtube', 'v3', developerKey=api_key)

	# retrieve youtube video results
	video_response = youtube.commentThreads().list(part='snippet,replies', videoId=video_id).execute()

	# iterate video response
	while video_response:

		# extracting required info
		# from each result object
		for item in video_response['items']:

			# Extracting comments ()
			published = item['snippet']['topLevelComment']['snippet']['publishedAt']
			user = item['snippet']['topLevelComment']['snippet']['authorDisplayName']

			# Extracting comments
			comment = item['snippet']['topLevelComment']['snippet']['textDisplay']
			likeCount = item['snippet']['topLevelComment']['snippet']['likeCount']

			replies.append([published, user, comment, likeCount])

			# counting number of reply of comment
			replycount = item['snippet']['totalReplyCount']

			# if reply is there
			if replycount>0:
				# iterate through all reply
				for reply in item['replies']['comments']:

					# Extract reply
					published = reply['snippet']['publishedAt']
					user = reply['snippet']['authorDisplayName']
					repl = reply['snippet']['textDisplay']
					likeCount = reply['snippet']['likeCount']

					# Store reply is list
					#replies.append(reply)
					replies.append([published, user, repl, likeCount])

			# print comment with list of reply
			#print(comment, replies, end = '\n\n')

			# empty reply list
			#replies = []

		# Again repeat
		if 'nextPageToken' in video_response:
			video_response = youtube.commentThreads().list(
					part = 'snippet,replies',
					pageToken = video_response['nextPageToken'],
					videoId = video_id
				).execute()
		else:
			break
	#endwhile
	return replies

# api key
api_key = 'AIzaSyCx5pWDM6VDEHYiAZlRh35qhDW9gEwAr3s'
video_id = "UQtvZgKmGJo" #isikan dengan kode / ID video

# Call function
comments = video_comments(video_id)

comments

df = pd.DataFrame(comments, columns=['Waktu', 'Penulis', 'Komentar', 'Suka'])
df

len(df.index)

df[['Waktu', 'Penulis', 'Komentar', 'Suka']].head()

df_new = df[['Waktu', 'Penulis', 'Komentar', 'Suka']]
df_sorted= df_new.sort_values(by='Waktu', ascending=True)
df_sorted.head()

df_scrape = df_sorted[['Waktu', 'Penulis', 'Komentar', 'Suka']]

df_scrape.head()

df_scrape.to_csv("yt_data.csv", index = False)

"""**Filtering**"""

!pip install Sastrawi

!pip install vaderSentiment

df01 = pd.read_csv('yt_data.csv')
df01.head(5)

df01.shape

#menambah variabel "label"
Sentiment = []
for index, row in df01.iterrows():
    if row['Suka'] > 5:
        Sentiment.append('Positif')
    elif row['Suka'] >= 3 and row['Suka'] < 5:
        Sentiment.append('Netral')
    else:
        Sentiment.append('Negatif')

df01['Sentiment'] = Sentiment
df01.head()

df01.shape

#menghilangkan variabel yang tidak dipakai
df01_data = df01.copy()
df01_data = df01.drop(columns= ['Waktu', 'Penulis'])
df01_data.head()

df01_data.shape

x = df01_data.iloc[:, 0].values
y = df01_data.iloc[:, -1].values

#memecah data test 30% dari  keseluruhan data
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state=0)

x_train

len(x_train)

len(x)

x_test

len(x_test)

nltk.download('punkt')
nltk.download('stopword')
nltk.download('wordnet') #stemming

df01_data.Komentar

#clearning
#remove url
df01_data['Komentar'] =  df01_data['Komentar'].str.replace('https\S', '', case=False) #tokenizing

#ubah teks jadi huruf kecil
df01_data['Komentar'] =  df01_data['Komentar'].str.lower() #case folding

#remove mention
df01_data['Komentar'] =  df01_data['Komentar'].str.replace('@\S+', '', case=False) #tokenizing

#remove hastag
df01_data['Komentar'] =  df01_data['Komentar'].str.replace('#\S+', '', case=False) #tokenizing

#remove next character
df01_data['Komentar'] =  df01_data['Komentar'].str.replace("\'W+", '', case=False) #tokenizing

#remove punctuation
df01_data['Komentar'] =  df01_data['Komentar'].str.replace('[^\w\s]', '', case=False) #tokenizing

#remove number
df01_data['Komentar'] =  df01_data['Komentar'].str.replace(r'w*\d+\w*', '', case=False) #tokenizing

#remove spasi berlebih
df01_data['Komentar'] =  df01_data['Komentar'].str.replace('\s(2)', '', case=False) #tokenizing

df01_data.Komentar

#tokenizing (proses penguraian deskripsi dari kalimat jadi kata)
#testing
from nltk.tokenize import word_tokenize

x = df01_data.iloc[0]
print(nltk.word_tokenize(x['Komentar']))

def identify_token(row) :
  text = row['Komentar']
  tokens = nltk.word_tokenize(text)
  token_words = [w for w in tokens if w.isalpha()]
  return token_words

df01_data['Komentar'] = df01_data.apply(identify_token, axis = 1)
df01_data.Komentar

#stemming (kata dasar) (tahap cari root kata dari kata filtering)
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
stemming = PorterStemmer()

def stem_list(row) :
  text =  row ['Komentar']
  stem = [stemming.stem(word) for word in text]
  return(stem)

df01_data['Komentar'] = df01_data.apply(stem_list, axis=1)
df01_data.Komentar

#stopword (hapus kata tidak penting)
from nltk.corpus import  stopwords
import nltk
nltk.download('stopwords')
#from nltk.tokenize import word_tokenize
stops =  set (stopwords.words ('indonesian'))

df01_data.head()

df01_data['Sentiment'].value_counts()

"""**Term Weighting**"""

#menghitung vector

Komentar = df01['Komentar']

tfidfvectorizer = TfidfVectorizer(analyzer='word')
tfidf_wm = tfidfvectorizer.fit_transform(Komentar)
tfidf_tokens = tfidfvectorizer.get_feature_names_out()
df_tfidfvect = pd.DataFrame.sparse.from_spmatrix(tfidf_wm, index=Komentar.index, columns=tfidf_tokens)

print("\nTD-IDF Vectorizer\n")
print(df_tfidfvect)

#mengubah jadi vector term
tv =  TfidfVectorizer()
X_train = tv.fit_transform(x_train)
X_test = tv.transform(x_test)

X_test[1:1]

#menggunakan perhitungan tf idf
x = df01_data['Sentiment']
y = df01_data['Sentiment']

x_train, x_test, y_train, y_test  = train_test_split(x, y, test_size = 0.3, random_state=0)

print("Matriks TF-IDF data pelatihan:")
print(X_train.toarray())

print("Matriks TF-IDF data uji:")
print(X_test.toarray())

x_train

len(x_train)

x_test

len(x_test)

"""Menentukan Klasifikasi K-NN"""

# Pembuatan model
classifier = KNeighborsClassifier(n_neighbors=3)
classifier.fit(X_train, y_train)

# Prediksi menggunakan data test
y_pred = classifier.predict(X_test)

!pip install seaborn matplotlib

# Hitung akurasi
accuracy_score = metrics.accuracy_score(y_test, y_pred)
accuracy_score = round(accuracy_score * 100, 2)
print('Accuracy: ' + str(accuracy_score) + '%')

#menghitung presisi
macro_precision = (metrics.precision_score(y_test, y_pred, average='macro'))
macro_precision = round(macro_precision * 100, 2)
print('Precision : ' + str(macro_precision) +'%')

#menghitung recall
macro_recall = (metrics.recall_score(y_test, y_pred, average='macro'))
macro_recall = round(macro_recall * 100, 2)
print('Recall : '+str(macro_recall) +'%')

#menghitung f1-score
macro_f1 = metrics.f1_score(y_test, y_pred, average='macro')
macro_f1_percentage = round(macro_f1 * 100, 2)
print('F1 : ' + str(macro_f1_percentage) +'%')

# Menghitung confusion matrix
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

labels = ['Positif', 'Netral', 'Negatif']

conf_matrix = np.array([[1956, 7, 3],
                        [100, 1, 0],
                        [183, 4, 2]])

# Buat DataFrame dari confusion matrix
conf_matrix_df = pd.DataFrame(conf_matrix, index=labels, columns=labels)

custom_palette = sns.color_palette(["#00796B", "#4CAF50"])

# Plotting the heatmap
plt.figure(figsize=(6, 4))
sns.set(font_scale=1)
heatmap = sns.heatmap(conf_matrix_df, annot=True, fmt="d", cmap=custom_palette, linewidths=.5)

# Tampilkan tulisan pada heatmap
for i in range(len(labels)):
    for j in range(len(labels)):
        if conf_matrix_df.iloc[i, j] > 0:
            heatmap.text(j + 0.5, i + 0.5, str(conf_matrix_df.iloc[i, j]),
                         ha='center', va='center', color='black')

plt.title('Confusion Matrix')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()